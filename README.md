# Latent-To-Latent
Unofficial PyTorch implementation of "Latent to Latent“

![](https://github.com/850552586/Latent-To-Latent/blob/main/assets/smile.png)

----------------------------------------------------------------------------------

> [Latent-To-Latent: A Learned Mapper for Identity Preserving Editing of Multiple Face Attributes in StyleGAN-generated Images](https://openaccess.thecvf.com/content/WACV2022/papers/Khodadadeh_Latent_to_Latent_A_Learned_Mapper_for_Identity_Preserving_Editing_WACV_2022_paper.pdf)

> **Abstract:** Several recent papers introduced techniques to adjust the attributes of human faces generated by unconditional GANs such as StyleGAN. Despite efforts to disentangle the attributes, a request to change one attribute often triggers unwanted changes to other attributes as well. More importantly, in some cases, a human observer would not recognize the edited face to belong to the same person. We propose an approach where a neural network takes as input the latent encoding of a face and the desired attribute changes and outputs the latent space encoding of the edited image. The network is trained offline using unsupervised data, with training labels generated by an off-the-shelf attribute classifier. The desired attribute changes and conservation laws, such as identity maintenance, are encoded in the training loss. The number of attributes the mapper can simultaneously modify is only limited by the attributes available to the classifier – we trained a network that handles 35 attributes, more than any previous approach. As no optimization is performed at deployment time, the computation time is negligible, allowing real-time attribute editing. Qualitative and quantitative comparisons with the current state-of-the-art show our method is better at conserving the identity of the face and restricting changes to the requested attributes.

![image-20220419112318402](https://github.com/850552586/Latent-To-Latent/blob/main/assets/image-20220419112318402.png)

--------------------------------------------

## Pre-trained Networks

| Path                                                         | description                                                  |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [Restyle_e4e_ffhq_encode](https://drive.google.com/file/d/1e2oXVeBPXMQoUoC_4TNwAWpOPpSEhE_e/view?usp=sharing) | ReStyle applied over e4e trained on the [FFHQ](https://github.com/NVlabs/ffhq-dataset) dataset. |
| [BranchedTiny](链接：https://pan.baidu.com/s/1aOvZjYG6xepgCr0B6cnvJA?pwd=hwv6 <br/>提取码：hwv6 <br/>--来自百度网盘超级会员V1的分享) | pretrained model for Attribute regressor                     |
| [mobile_facenet](链接：https://pan.baidu.com/s/1edI6edYuDaeiTKWzDs4JnQ?pwd=r6dn <br/>提取码：r6dn <br/>--来自百度网盘超级会员V1的分享) | pretrained model for face_extractor                          |

The pre-training files under `mobile_facenet `need to be placed in the project `utils/FaceExtractor/checkpoint` directory

`BranchedTiny.ckpt` needs to be placed in the project `models\attribute_classifier` directory

--------------------------------------------------------------------

## Training

I have trained smile and bushy_eyebrows attributes on FFHQ dataset. The remaining attributes can be trained by themselves.

```shell
python trainer.py \
--exp_dir ffhq
--attribute young
--gpu 0
--epoch 10
--batch_size 8
```

## Testing

**Inference**

```shell
python inference.py \
--attribute smiling \
--gpu 0 \
--ckpt ./pretrained/smiling.pt \
--exdir /home/xujiamu/data/tmp \
--savedir /home/xujiamu/data/result
```

## Other

Since the project is not open source, and the author has too little description of the model in the paper, the implementation of some modules in the model is my own replacement.

**（1）Attribute regressor**

The author used Microsoft API to generate data attribute scores for a large number of images and trained attribute classifiers. I used an open source attribute classifier as an alternative.

**（2） Latent-To-Latent Model**

The authors only describe the use of a fully connected network, but do not go into detail. In my experimental scheme, I used a 2-layer fully connected network, and at the same time borrowed from StyleClip's scheme and used an inverted pyramid structure.

If you have a better solution or suggestion, please post an issue message.

